{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Recommendations:  \n",
      "\n",
      "The Drake Hotel: Canadian, Indian; Khao San Road: Thai; Agave y Aguacate: Mexican; Byblos: Middle Eastern; Lee Garden: Chinese; Ufficio: Italian.\n",
      "Movie Recommendations: \n",
      "\n",
      "1. Brooklyn: A romantic drama which follows an Irish immigrant in 1950s New York, exploring themes of identity and belonging. \n",
      "2. The Imitation Game: A biographical drama about Alan Turing, a British mathematician, exploring themes of genius, courage, and prejudice. \n",
      "3. The Revenant: A gritty survival drama set in the American West, exploring themes of revenge and survival. \n",
      "4. The Grand Budapest Hotel: A comedic and visually stunning romp through\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    Authorization= 'Bearer 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'\n",
    "}\n",
    "\n",
    "\n",
    "# Replace these with your own data or another way to get them\n",
    "age = \"25\"\n",
    "gender = \"male\"\n",
    "name = \"John\"\n",
    "nationality = \"Canadian\"\n",
    "\n",
    "\n",
    "# Restaurant Recommendations\n",
    "restaurant_data = {\n",
    "    'prompt': f'You are a {age}-year-old {gender} named {name}. Based on the following probabilities of potential nationalities: {nationality}, I recommend restaurants that cater to the listed ethnicities. Please suggest some restaurants that offer a variety of options and can provide a memorable dining experience for someone with your profile. You live in Toronto, ON. INCLUDE LOCATIONS OF RESTAURANTS AND WHICH ETHIC CUISINE IT IS [Keep the response concise, limiting it to a maximum of 30 words.]',\n",
    "    'model': 'text-davinci-003',\n",
    "    'max_tokens': 100,\n",
    "    'temperature': 0.7,\n",
    "    'n': 1,\n",
    "    'stop': None\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.openai.com/v1/completions', headers=headers, json=restaurant_data)\n",
    "restaurant_recommendations = response.json()[\"choices\"][0][\"text\"]\n",
    "print(\"Restaurant Recommendations:\", restaurant_recommendations)\n",
    "\n",
    "# Movie Recommendations\n",
    "movie_data = {\n",
    "    'prompt': f'You are a {age}-year-old {gender} named {name}. Based on the following probabilities of potential nationalities: {nationality}, I recommend movies that cater to diverse genres and preferences. Please suggest some movies that cater to said age gender and ethnicity and can provide an enjoyable viewing experience for someone with your profile. For each movie give a short explanation as to why[Keep the response concise, limiting it to a maximum of 30 words.]',\n",
    "    'model': 'text-davinci-003',\n",
    "    'max_tokens': 100,\n",
    "    'temperature': 0.7,\n",
    "    'n': 1,\n",
    "    'stop': None\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.openai.com/v1/completions', headers=headers, json=movie_data)\n",
    "movie_recommendations = response.json()[\"choices\"][0][\"text\"]\n",
    "print(\"Movie Recommendations:\", movie_recommendations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a model on Hugging Face involves training a pre-trained model on your specific dataset or task. Here's a step-by-step guide on how to do it:\n",
    "\n",
    "### 1. Install the Required Libraries:\n",
    "\n",
    "If you haven't already, install the `transformers` and `datasets` libraries from Hugging Face:\n",
    "\n",
    "\n",
    "### 2. Load Your Dataset:\n",
    "\n",
    "You can use the `datasets` library from Hugging Face to load your dataset, or you can use your own data in a format like CSV or JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_hf_folder.py:95: UserWarning: A token has been found in `C:\\Users\\Samsickle\\.huggingface\\token`. This is the old path where tokens were stored. The new location is `C:\\Users\\Samsickle\\.cache\\huggingface\\token` which is configurable using `HF_HOME` environment variable. Your token has been copied to this new location. You can now safely delete the old token file manually or use `huggingface-cli logout`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e77258da0df408395a2b8fba6201aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e6a315fda34213b2b4e5e34e2c9b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24198d56ec7a4c3aaa34dbdb6b297be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to C:/Users/Samsickle/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6e68797c274fcea818e29fdb1200c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab997ce180894b949b475ed693aab58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e397b347080f479eb6c9f6f247351f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce78837dfe524f3aa91272b148374631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to C:/Users/Samsickle/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57995367d2ee43428d94e82765ae9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Load the Tokenizer and Model:\n",
    "\n",
    "Choose the pre-trained model you want to fine-tune. For instance, if you're fine-tuning a BERT model for text classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985ffee62bfc44ea9e3589bdc2e06fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Samsickle\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dbadff1a4e4991a4df8c4f4989c82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6096c30b324d425097fc6685504c09bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8864954c42c7440ea8690c412cf2c012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Tokenize Your Dataset:\n",
    "\n",
    "You need to tokenize your data to convert it into a format that the model can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d139f5e9583544a9816f71bd21afb012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Prepare for Training:\n",
    "\n",
    "You'll need to define a training argument, a training data loader, and a trainer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Fine-Tune the Model:\n",
    "\n",
    "With everything set up, you can now fine-tune your model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Evaluate the Model:\n",
    "\n",
    "After training, you can evaluate your model to see how well it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = Trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Save and Use Your Fine-Tuned Model:\n",
    "\n",
    "Once fine-tuned, you can save your model and then use it for inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./my_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./my_finetuned_model\")\n",
    "\n",
    "# To use later:\n",
    "# model = BertForSequenceClassification.from_pretrained(\"./my_finetuned_model\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"./my_finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** The above steps are a general process for fine-tuning using Hugging Face. Depending on your specific task or dataset, you might need to make adjustments. Also, make sure to adjust hyperparameters, like learning rate or number of epochs, according to your dataset's size and complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
